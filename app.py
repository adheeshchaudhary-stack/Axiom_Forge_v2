import os
import random
from datetime import datetime
from io import BytesIO

import pandas as pd
import plotly.graph_objects as go
import streamlit as st
import pdfplumber
from dotenv import load_dotenv
from fpdf import FPDF
from groq import Groq

from forensics.forensic_tools import (
    calculate_file_hashes,
    extract_pdf_metadata,
    extract_image_metadata,
    get_file_type
)


load_dotenv()

DEMO_USERNAME = "Admin"
DEMO_PASSWORD = "Axiom99"


class AxiomPDF(FPDF):
    def footer(self) -> None:
        self.set_y(-15)
        self.set_font("Helvetica", "I", 8)
        self.cell(
            0,
            10,
            "Generated by Axiom Forensics - Confidential M&A Report",
            0,
            0,
            "C",
        )


def check_password() -> bool:
    """
    Simple password gate for the dashboard.
    """
    def password_entered():
        username = st.session_state.get("username_input", "")
        password = st.session_state.get("password", "")
        if username == DEMO_USERNAME and password == DEMO_PASSWORD:
            st.session_state["password_correct"] = True
            # Do not keep the raw secrets in memory
            st.session_state.pop("password", None)
            st.session_state.pop("username_input", None)
        else:
            st.session_state["password_correct"] = False

    if st.session_state.get("password_correct", False):
        return True

    st.sidebar.markdown("### Login")
    st.sidebar.text_input(
        "Username",
        key="username_input",
    )
    st.sidebar.text_input(
        "Password",
        type="password",
        key="password",
        on_change=password_entered,
    )

    if st.session_state.get("password_correct") is False:
        st.sidebar.error("Access Denied. Incorrect password.")

    st.sidebar.caption(
        "Forgot Password? Please contact the Axiom System Administrator for access credentials."
    )

    return False


def get_ai_insights(
    result: dict,
    fraud_df: pd.DataFrame,
    dataset_label: str,
    row_count: int,
) -> str:
    """
    Call Groq to generate portfolio-level AI insights using GROQ_API_KEY.
    """
    api_key = os.environ.get("GROQ_API_KEY")
    if not api_key:
        return (
            "Axiom AI Insights unavailable: GROQ_API_KEY is not configured. "
            "Please set it in your environment or .env file."
        )

    client = Groq(api_key=api_key)

    summary = (
        f"Dataset: {dataset_label}\n"
        f"Rows analyzed: {row_count}\n"
        f"Total rows: {result['total_rows']}, "
        f"fraud rows: {result['fraud_rows']}, "
        f"integrity score: {result['integrity_score']:.2f}%."
    )

    fraud_examples = ""
    if not fraud_df.empty:
        # Compute latency for anomaly analysis
        tmp = fraud_df.copy()
        tmp["latency"] = tmp["payment_time"] - tmp["lead_time"]

        lines: list[str] = []

        if row_count > 500:
            # Smart sampling for large datasets: summarize instead of listing everything
            fraud_rate = (
                result["fraud_rows"] / result["total_rows"]
                if result["total_rows"]
                else 0.0
            )
            avg_latency = tmp["latency"].mean()
            lines.append(
                "Fraud summary on large dataset:\n"
                f"- total_rows={result['total_rows']}\n"
                f"- fraud_rows={result['fraud_rows']}\n"
                f"- fraud_rate={fraud_rate:.3f}\n"
                f"- avg_fraud_latency={avg_latency:.3f}s"
            )

            top_anomalies = tmp.sort_values("latency", ascending=False).head(5)
            lines.append("Top 5 latency anomalies among fraud-flagged rows:")
            for _, row in top_anomalies.iterrows():
                lines.append(
                    f"- Transaction {int(row['transaction_id'])}: "
                    f"lead_time={row['lead_time']}, payment_time={row['payment_time']}, "
                    f"latency={row['latency']:.3f}s"
                )
        else:
            # Smaller datasets: list up to 5 illustrative fraud cases
            sampled = tmp.head(5)
            for _, row in sampled.iterrows():
                lines.append(
                    f"- Transaction {int(row['transaction_id'])}: "
                    f"lead_time={row['lead_time']}, payment_time={row['payment_time']}, "
                    f"latency={row['latency']:.3f}s"
                )

        fraud_examples = "\n".join(lines)

    prompt = f"""
You are the Axiom Forge AI Tribunal, comprising:
- A Forensic Prosecutor
- A Defense Attorney
- A Neutral Judge

Here is the portfolio latency summary:
{summary}

Here are sample fraud-flagged transactions (if any), including potential zero-latency paradox cases:
{fraud_examples or 'No fraud-flagged transactions.'}

Act in THREE clearly separated roles and produce a Markdown-formatted report:

### THE CASE FOR FRAUD
- As the Prosecutor, argue the strongest possible case that this dataset indicates fraud or synthetic manipulation.
- Explicitly discuss any evidence consistent with the "Zero-Latency Paradox" (very small time gaps like 0.1s) and how it might signal bot activity or data tampering.
- If available, refer to anomalous time patterns or volume spikes as if they came from heatmaps or density analyses.
- Mention how a Benford's Law analysis of transaction amounts might further support suspicions (even if you must speak hypothetically based on typical fraud signatures).

### POTENTIAL INNOCENT EXPLANATIONS
- As the Defense, list plausible benign explanations for these patterns.
- Include examples such as: automated payment gateways, batch processing windows, very fast legitimate systems, timezone quirks, or logging precision issues.
- Emphasize where the current evidence is ambiguous or could be misinterpreted.

### FINAL VERDICT
- As the Judge, weigh both sides and state a clear verdict (e.g. "Elevated Fraud Risk", "Moderate Concern", or "Low Apparent Risk").
- Base this verdict on the balance of evidence, noting both the strength of the Latency Paradox/Benford-style concerns and the credibility of innocent explanations.
- Close with one concrete next step you would recommend for the deal or risk team.
"""

    completion = client.chat.completions.create(
        model="llama-3.3-70b-versatile",
        messages=[
            {
                "role": "system",
                "content": "You are a precise M&A and fraud risk analyst.",
            },
            {"role": "user", "content": prompt},
        ],
        temperature=0.3,
    )

    return completion.choices[0].message.content.strip()


def get_direct_ai_insights(
    df_string: str,
    dataset_label: str,
    row_count: int,
) -> str:
    """
    Call Groq to generate direct forensic AI insights using GROQ_API_KEY.
    Bypasses the complex audit engine and analyzes the raw dataframe directly.
    """
    api_key = os.environ.get("GROQ_API_KEY")
    if not api_key:
        return (
            "Axiom AI Insights unavailable: GROQ_API_KEY is not configured. "
            "Please set it in your environment or .env file."
        )

    client = Groq(api_key=api_key)

    prompt = f"""
You are a forensic investigator analyzing this dataset. Look at this data and find any anomalies in dates or locations.

Dataset: {dataset_label}
Rows analyzed: {row_count}

Here is the complete dataset:
{df_string}

Please analyze this data and provide a detailed forensic report focusing on:
1. Any suspicious patterns in dates/times
2. Anomalies in location data
3. Unusual transaction patterns
4. Any other red flags that might indicate fraud or data manipulation

Provide your analysis in a clear, structured format with specific examples from the data.
"""

    completion = client.chat.completions.create(
        model="llama-3.3-70b-versatile",
        messages=[
            {
                "role": "system",
                "content": "You are a forensic investigator specializing in financial data analysis.",
            },
            {"role": "user", "content": prompt},
        ],
        temperature=0.3,
    )

    return completion.choices[0].message.content.strip()


def get_chat_ai_response(prompt: str) -> str:
    """
    Call Groq to generate chat responses using GROQ_API_KEY.
    Connects to Llama 3.3 model for conversational AI responses.
    """
    api_key = os.environ.get("GROQ_API_KEY")
    if not api_key:
        return (
            "Axiom AI Chat unavailable: GROQ_API_KEY is not configured. "
            "Please set it in your environment or .env file."
        )

    client = Groq(api_key=api_key)

    completion = client.chat.completions.create(
        model="llama-3.3-70b-versatile",
        messages=[
            {
                "role": "system",
                "content": "You are a forensic investigator AI assistant. Provide helpful, accurate, and professional responses to forensic analysis questions. Focus on data analysis, pattern recognition, and anomaly detection in financial and transactional data.",
            },
            {"role": "user", "content": prompt},
        ],
        temperature=0.3,
    )

    return completion.choices[0].message.content.strip()


def infer_forensic_columns(df: pd.DataFrame) -> dict:
    """
    Best-effort mapping from arbitrary column names to our forensic engine schema.

    Looks for keywords like 'time', 'date', 'amount', 'id', and 'user' and
    returns a mapping for at least:
      - transaction_id (if possible)
      - lead_time (first time/date-like column)
      - payment_time (second time/date-like column)
    """
    columns_lower = {c.lower(): c for c in df.columns}

    # Detect ID-like column (any column containing 'id' or 'trans')
    txn_id_col = None
    for key, orig in columns_lower.items():
        if "id" in key or "trans" in key:
            txn_id_col = orig
            break

    # Detect time/date-like columns (including explicit known names)
    time_cols = [
        orig
        for key, orig in columns_lower.items()
        if ("time" in key) or ("date" in key)
    ]

    mapping: dict[str, str | None] = {
        "transaction_id": txn_id_col,
        "lead_time": None,
        "payment_time": None,
    }

    # Prefer explicit Request_Start / Completion_Timestamp style names when present
    for key, orig in columns_lower.items():
        if "request_start" in key:
            mapping["lead_time"] = orig
        if "completion_timestamp" in key:
            mapping["payment_time"] = orig

    # Fallback to first/second time-like columns if still unset
    if mapping["lead_time"] is None and len(time_cols) >= 1:
        mapping["lead_time"] = time_cols[0]
    if mapping["payment_time"] is None and len(time_cols) >= 2:
        mapping["payment_time"] = time_cols[1]

    return mapping


def _benford_summary(df: pd.DataFrame) -> str:
    """
    Compute a simple Benford-style summary over any amount-like column, if present.
    """
    amount_col = None
    for col in df.columns:
        lower = col.lower()
        if "amount" in lower or "amt" in lower or "value" in lower:
            amount_col = col
            break

    if amount_col is None:
        return "Benford's Law: Skipped (no amount/value column detected)."

    series = pd.to_numeric(df[amount_col], errors="coerce").dropna()
    if series.empty:
        return "Benford's Law: Skipped (amount column contains no numeric values)."

    first_digits = series.astype(str).str.lstrip("0.-").str[0].astype(int)
    counts = first_digits.value_counts(normalize=True).sort_index()

    theoretical = {d: (pd.np.log10(1 + 1 / d)) for d in range(1, 10)}

    lines = [f"Benford's Law on column '{amount_col}':"]
    for d in range(1, 10):
        observed = counts.get(d, 0.0)
        expected = theoretical[d]
        lines.append(
            f"  Digit {d}: observed={observed:.3f}, expected={expected:.3f}, "
            f"delta={observed - expected:+.3f}"
        )

    return "\n".join(lines)


def build_audit_pdf(
    result: dict,
    ai_text: str,
    fraud_df: pd.DataFrame,
    full_df: pd.DataFrame | None = None,
    potential_haircut: int = 0,
) -> bytes:
    """
    Build the official Axiom Forensics Truth OS PDF report.
    """
    pdf = AxiomPDF()
    pdf.set_auto_page_break(auto=True, margin=15)
    pdf.add_page()

    # Title
    pdf.set_font("Helvetica", "B", 16)
    pdf.cell(0, 10, "AXIOM FORENSICS - TRUTH OS REPORT", ln=True, align="C")
    pdf.ln(5)

    # Summary section
    pdf.set_font("Helvetica", "B", 12)
    pdf.cell(0, 8, "Summary", ln=True)
    pdf.set_font("Helvetica", "", 11)
    pdf.multi_cell(
        0,
        6,
        f"Portfolio Integrity Score: {result['integrity_score']:.2f}%\n"
        f"Total Transactions: {result['total_rows']}\n"
        f"Fraud-Flagged Transactions: {result['fraud_rows']}\n",
    )
    pdf.ln(4)

    # Forensic Valuation Summary
    pdf.set_font("Helvetica", "B", 12)
    pdf.cell(0, 8, "Forensic Valuation Summary", ln=True)
    pdf.set_font("Helvetica", "", 11)
    total_deal_value = 300_000_000  # $300M
    pdf.multi_cell(
        0,
        6,
        f"Total Deal Value: ${total_deal_value:,.0f}\n"
        f"Potential Haircut: ${potential_haircut:,.0f}\n"
        f"Haircut Percentage: {(potential_haircut / total_deal_value * 100):.2f}%\n"
        f"Risk Level: {'HIGH' if potential_haircut > 10_000_000 else 'MODERATE' if potential_haircut > 0 else 'LOW'}",
    )
    pdf.ln(4)

    # Mathematical Findings
    pdf.set_font("Helvetica", "B", 12)
    pdf.cell(0, 8, "Mathematical Findings", ln=True)
    pdf.set_font("Helvetica", "", 11)
    benford_text = (
        _benford_summary(full_df) if full_df is not None else "Benford's Law: Dataset not provided."
    )
    pdf.multi_cell(0, 6, benford_text)
    pdf.ln(4)

    # Adversarial AI Report
    pdf.set_font("Helvetica", "B", 12)
    pdf.cell(0, 8, "Adversarial AI Report", ln=True)
    pdf.set_font("Helvetica", "", 11)
    pdf.multi_cell(0, 6, ai_text)
    pdf.ln(4)

    # Evidence section
    pdf.set_font("Helvetica", "B", 12)
    pdf.cell(0, 8, "Evidence: Flagged Transactions", ln=True)
    pdf.set_font("Helvetica", "", 11)

    if fraud_df.empty:
        pdf.multi_cell(0, 6, "No fraud-flagged transactions in this portfolio.")
    else:
        flagged_ids = (
            fraud_df["transaction_id"].astype(int).head(10).tolist()
        )
        ids_str = ", ".join(str(tid) for tid in flagged_ids)
        pdf.multi_cell(
            0,
            6,
            "Top flagged Transaction IDs (max 10): " + ids_str,
        )

    # Export PDF to bytes
    pdf_bytes = pdf.output(dest="S").encode("latin-1")
    return pdf_bytes


def _handle_login():
    """Handle login logic"""
    username = st.session_state.get("username_input", "")
    password = st.session_state.get("password", "")
    if username == DEMO_USERNAME and password == DEMO_PASSWORD:
        st.session_state["password_correct"] = True
        # Do not keep the raw secrets in memory
        st.session_state.pop("password", None)
        st.session_state.pop("username_input", None)
    else:
        st.session_state["password_correct"] = False


def main():
    st.set_page_config(page_title="Axiom Forge Truth OS", layout="centered")

    # Initialize chat messages in session state if not already present
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # Authentication check at the very beginning
    if not st.session_state.get("password_correct", False):
        st.title("Axiom Forge Truth OS")
        st.subheader("Chronos-Audit Portfolio Dashboard")
        
        # Show login form
        st.sidebar.markdown("### Login")
        st.sidebar.text_input(
            "Username",
            key="username_input",
        )
        st.sidebar.text_input(
            "Password",
            type="password",
            key="password",
            on_change=_handle_login
        )

        if st.session_state.get("password_correct") is False and st.session_state.get("password"):
            st.sidebar.error("Access Denied. Incorrect password.")

        st.sidebar.caption(
            "Forgot Password? Please contact the Axiom System Administrator for access credentials."
        )

        st.write(
            "Status: ðŸ”’ Locked. Enter the access password in the sidebar to unlock "
            "the Axiom Forensics dashboard."
        )
        
        # Stop execution if not authenticated
        st.stop()

    # Dashboard logic only runs if authenticated
    st.title("Axiom Forge Truth OS")
    st.subheader("Direct Forensic Analysis")

    st.write(
        "Upload a CSV or PDF file to perform direct forensic analysis bypassing complex audit logic."
    )

    uploaded_file = st.sidebar.file_uploader(
        "Upload CSV or PDF for Direct Analysis", type=["csv", "pdf"]
    )

    # ðŸ› ï¸ Forensic Toolbox Section
    if uploaded_file is not None:
        st.markdown("---")
        st.subheader("ðŸ› ï¸ Forensic Metadata & Integrity")
        
        # Calculate file hashes
        file_content = uploaded_file.getvalue()
        hashes = calculate_file_hashes(file_content)
        
        # Display file integrity information
        col1, col2 = st.columns(2)
        with col1:
            st.write("**File Integrity Hashes**")
            st.code(f"MD5: {hashes['md5']}")
            st.code(f"SHA-256: {hashes['sha256']}")
        
        # Extract metadata based on file type
        file_type = get_file_type(file_content, uploaded_file.name)
        
        if file_type == 'pdf':
            with col2:
                st.write("**PDF Metadata**")
                pdf_metadata = extract_pdf_metadata(file_content)
                if 'error' in pdf_metadata:
                    st.error(pdf_metadata['error'])
                else:
                    metadata_text = ""
                    if pdf_metadata['author']:
                        metadata_text += f"**Author:** {pdf_metadata['author']}\n"
                    if pdf_metadata['creator']:
                        metadata_text += f"**Creator:** {pdf_metadata['creator']}\n"
                    if pdf_metadata['creation_date']:
                        metadata_text += f"**Creation Date:** {pdf_metadata['creation_date']}\n"
                    if pdf_metadata['title']:
                        metadata_text += f"**Title:** {pdf_metadata['title']}\n"
                    if pdf_metadata['page_count']:
                        metadata_text += f"**Page Count:** {pdf_metadata['page_count']}\n"
                    
                    if metadata_text:
                        st.markdown(metadata_text)
                    else:
                        st.info("No metadata found in this PDF.")
        
        elif file_type == 'image':
            with col2:
                st.write("**Image Metadata (EXIF)**")
                image_metadata = extract_image_metadata(file_content)
                if 'error' in image_metadata:
                    st.error(image_metadata['error'])
                else:
                    metadata_text = ""
                    if image_metadata['format']:
                        metadata_text += f"**Format:** {image_metadata['format']}\n"
                    if image_metadata['size']:
                        metadata_text += f"**Size:** {image_metadata['size'][0]}x{image_metadata['size'][1]}\n"
                    if image_metadata['mode']:
                        metadata_text += f"**Color Mode:** {image_metadata['mode']}\n"
                    
                    if metadata_text:
                        st.markdown(metadata_text)
                    
                    # Show some key EXIF data
                    exif_data = image_metadata.get('exif_data', {})
                    if exif_data:
                        st.write("**Key EXIF Data:**")
                        key_exif = ['DateTime', 'Make', 'Model', 'Software', 'GPSInfo']
                        exif_text = ""
                        for key in key_exif:
                            if key in exif_data:
                                exif_text += f"**{key}:** {exif_data[key]}\n"
                        
                        if exif_text:
                            st.markdown(exif_text)
                        else:
                            st.info("No key EXIF data found.")
                    else:
                        st.info("No EXIF data found in this image.")
        
        else:
            with col2:
                st.info(f"File type '{file_type}' not supported for metadata extraction. Hashes calculated for integrity verification.")

    # Add a "Run Direct Analysis" button
    if st.button("Run Direct Forensic Analysis"):
        if uploaded_file is None:
            st.warning("Please upload a CSV file first.")
            return

        try:
            # Direct Data Load: Read CSV directly into dataframe
            df = pd.read_csv(uploaded_file)
            
            if df.empty:
                st.error("The uploaded CSV contains no rows. Please provide a file with data.")
                return

            data_label = f"Uploaded file: {uploaded_file.name}"

            st.success("CSV loaded successfully.")

            # The Forensic View: Create a simple table to display this data
            st.markdown("---")
            st.subheader("Data Preview")
            st.caption(data_label)
            st.dataframe(df, use_container_width=True)

            # Bypass the Engine: Skip main.py and timestamp_check.py completely
            # Direct AI Analysis: Send the entire dataframe as a string directly to get_ai_insights
            
            # Convert dataframe to string for AI analysis
            df_string = df.to_string()
            
            # Create a simplified result structure for the AI function
            result = {
                'total_rows': len(df),
                'fraud_rows': 0,  # We're not running the fraud detection engine
                'integrity_score': 100.0,  # Placeholder since we're bypassing the engine
            }
            
            # Create empty fraud_df since we're not running the fraud detection engine
            fraud_df = pd.DataFrame()
            
            with st.spinner("Consulting Axiom AI Brain for direct forensic analysis..."):
                # Direct AI Analysis: Send the entire dataframe as a string directly to the get_ai_insights function
                # Tell the AI: 'You are a forensic investigator. Look at this data and find any anomalies in dates or locations.'
                ai_text = get_direct_ai_insights(
                    df_string=df_string,
                    dataset_label=data_label,
                    row_count=len(df),
                )

            st.info(ai_text)

            st.markdown("---")
            st.subheader("Forensic Findings")

            if df.empty:
                st.info("No data to analyze.")
            else:
                # Display basic statistics
                st.write(f"**Total Rows:** {len(df)}")
                st.write(f"**Total Columns:** {len(df.columns)}")
                st.write(f"**Column Names:** {', '.join(df.columns)}")

                # Show data types
                st.write("**Data Types:**")
                st.write(df.dtypes)

                # Show basic statistics for numeric columns
                numeric_cols = df.select_dtypes(include=['number']).columns
                if len(numeric_cols) > 0:
                    st.write("**Basic Statistics for Numeric Columns:**")
                    st.write(df[numeric_cols].describe())

        except Exception as e:
            st.error(f"Error processing the uploaded file: {str(e)}")

    # Chat Interface Section
    st.markdown("---")
    st.subheader("ðŸ¤– Forensic AI Chat")

    # Display chat history
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # Chat input at the bottom
    if prompt := st.chat_input("Ask the Forensic AI a question about the data..."):
        # Add user message to chat history
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        # Display user message
        with st.chat_message("user"):
            st.markdown(prompt)

        # Generate AI response
        with st.chat_message("assistant"):
            with st.spinner("Analyzing your question..."):
                # Get current data context
                if uploaded_file is not None:
                    try:
                        file_content = uploaded_file.getvalue()
                        file_type = get_file_type(file_content, uploaded_file.name)
                        
                        if file_type == 'csv':
                            # Handle CSV files
                            df = pd.read_csv(uploaded_file)
                            df_string = df.to_string()
                            data_context = f"Current data ({uploaded_file.name}):\n{df_string}\n\n"
                        elif file_type == 'pdf':
                            # Handle PDF files with pdfplumber
                            with pdfplumber.open(BytesIO(file_content)) as pdf:
                                pdf_text = ""
                                for page in pdf.pages:
                                    text = page.extract_text()
                                    if text:
                                        pdf_text += text + "\n"
                            
                            data_context = f"Current PDF ({uploaded_file.name}):\n{pdf_text}\n\n"
                        else:
                            data_context = f"Current file ({uploaded_file.name}): Unsupported file type for text extraction.\n\n"
                    except Exception as e:
                        data_context = f"Error loading data: {str(e)}\n\n"
                else:
                    data_context = "No data uploaded yet.\n\n"

                # Combine data context with user question
                full_prompt = f"""You are a forensic investigator. Here is the current data context:

{data_context}

User question: {prompt}

Please provide a detailed forensic analysis focusing on the specific question asked. If no data is available, explain what analysis would be possible once data is uploaded."""

                # Get AI response using Groq
                ai_response = get_chat_ai_response(full_prompt)
                st.markdown(ai_response)
        
        # Add assistant response to chat history
        st.session_state.messages.append({"role": "assistant", "content": ai_response})

    # Global reset and logout controls at bottom of sidebar
    if st.sidebar.button("Clear All Data & Reset"):
        st.session_state.clear()
        st.rerun()

    if st.sidebar.button("Logout"):
        st.session_state.clear()
        st.rerun()


if __name__ == "__main__":
    main()

