import os
import random
from datetime import datetime
from io import BytesIO

import pandas as pd
import plotly.graph_objects as go
import streamlit as st
from dotenv import load_dotenv
from fpdf import FPDF
from groq import Groq

from forensics.timestamp_check import check_timestamps
from forensics.forensic_tools import (
    calculate_file_hashes,
    extract_pdf_metadata,
    extract_image_metadata,
    get_file_type
)
from main import run_portfolio_audit


load_dotenv()

DEMO_USERNAME = "Admin"
DEMO_PASSWORD = "Axiom99"


class AxiomPDF(FPDF):
    def footer(self) -> None:
        self.set_y(-15)
        self.set_font("Helvetica", "I", 8)
        self.cell(
            0,
            10,
            "Generated by Axiom Forensics - Confidential M&A Report",
            0,
            0,
            "C",
        )


def check_password() -> bool:
    """
    Simple password gate for the dashboard.
    """
    def password_entered():
        username = st.session_state.get("username_input", "")
        password = st.session_state.get("password", "")
        if username == DEMO_USERNAME and password == DEMO_PASSWORD:
            st.session_state["password_correct"] = True
            # Do not keep the raw secrets in memory
            st.session_state.pop("password", None)
            st.session_state.pop("username_input", None)
        else:
            st.session_state["password_correct"] = False

    if st.session_state.get("password_correct", False):
        return True

    st.sidebar.markdown("### Login")
    st.sidebar.text_input(
        "Username",
        key="username_input",
    )
    st.sidebar.text_input(
        "Password",
        type="password",
        key="password",
        on_change=password_entered,
    )

    if st.session_state.get("password_correct") is False:
        st.sidebar.error("Access Denied. Incorrect password.")

    st.sidebar.caption(
        "Forgot Password? Please contact the Axiom System Administrator for access credentials."
    )

    return False


def get_ai_insights(
    result: dict,
    fraud_df: pd.DataFrame,
    dataset_label: str,
    row_count: int,
) -> str:
    """
    Call Groq to generate portfolio-level AI insights using GROQ_API_KEY.
    """
    api_key = os.environ.get("GROQ_API_KEY")
    if not api_key:
        return (
            "Axiom AI Insights unavailable: GROQ_API_KEY is not configured. "
            "Please set it in your environment or .env file."
        )

    client = Groq(api_key=api_key)

    summary = (
        f"Dataset: {dataset_label}\n"
        f"Rows analyzed: {row_count}\n"
        f"Total rows: {result['total_rows']}, "
        f"fraud rows: {result['fraud_rows']}, "
        f"integrity score: {result['integrity_score']:.2f}%."
    )

    fraud_examples = ""
    if not fraud_df.empty:
        # Compute latency for anomaly analysis
        tmp = fraud_df.copy()
        tmp["latency"] = tmp["payment_time"] - tmp["lead_time"]

        lines: list[str] = []

        if row_count > 500:
            # Smart sampling for large datasets: summarize instead of listing everything
            fraud_rate = (
                result["fraud_rows"] / result["total_rows"]
                if result["total_rows"]
                else 0.0
            )
            avg_latency = tmp["latency"].mean()
            lines.append(
                "Fraud summary on large dataset:\n"
                f"- total_rows={result['total_rows']}\n"
                f"- fraud_rows={result['fraud_rows']}\n"
                f"- fraud_rate={fraud_rate:.3f}\n"
                f"- avg_fraud_latency={avg_latency:.3f}s"
            )

            top_anomalies = tmp.sort_values("latency", ascending=False).head(5)
            lines.append("Top 5 latency anomalies among fraud-flagged rows:")
            for _, row in top_anomalies.iterrows():
                lines.append(
                    f"- Transaction {int(row['transaction_id'])}: "
                    f"lead_time={row['lead_time']}, payment_time={row['payment_time']}, "
                    f"latency={row['latency']:.3f}s"
                )
        else:
            # Smaller datasets: list up to 5 illustrative fraud cases
            sampled = tmp.head(5)
            for _, row in sampled.iterrows():
                lines.append(
                    f"- Transaction {int(row['transaction_id'])}: "
                    f"lead_time={row['lead_time']}, payment_time={row['payment_time']}, "
                    f"latency={row['latency']:.3f}s"
                )

        fraud_examples = "\n".join(lines)

    prompt = f"""
You are the Axiom Forge AI Tribunal, comprising:
- A Forensic Prosecutor
- A Defense Attorney
- A Neutral Judge

Here is the portfolio latency summary:
{summary}

Here are sample fraud-flagged transactions (if any), including potential zero-latency paradox cases:
{fraud_examples or 'No fraud-flagged transactions.'}

Act in THREE clearly separated roles and produce a Markdown-formatted report:

### THE CASE FOR FRAUD
- As the Prosecutor, argue the strongest possible case that this dataset indicates fraud or synthetic manipulation.
- Explicitly discuss any evidence consistent with the "Zero-Latency Paradox" (very small time gaps like 0.1s) and how it might signal bot activity or data tampering.
- If available, refer to anomalous time patterns or volume spikes as if they came from heatmaps or density analyses.
- Mention how a Benford's Law analysis of transaction amounts might further support suspicions (even if you must speak hypothetically based on typical fraud signatures).

### POTENTIAL INNOCENT EXPLANATIONS
- As the Defense, list plausible benign explanations for these patterns.
- Include examples such as: automated payment gateways, batch processing windows, very fast legitimate systems, timezone quirks, or logging precision issues.
- Emphasize where the current evidence is ambiguous or could be misinterpreted.

### FINAL VERDICT
- As the Judge, weigh both sides and state a clear verdict (e.g. "Elevated Fraud Risk", "Moderate Concern", or "Low Apparent Risk").
- Base this verdict on the balance of evidence, noting both the strength of the Latency Paradox/Benford-style concerns and the credibility of innocent explanations.
- Close with one concrete next step you would recommend for the deal or risk team.
"""

    completion = client.chat.completions.create(
        model="llama-3.3-70b-versatile",
        messages=[
            {
                "role": "system",
                "content": "You are a precise M&A and fraud risk analyst.",
            },
            {"role": "user", "content": prompt},
        ],
        temperature=0.3,
    )

    return completion.choices[0].message.content.strip()


def infer_forensic_columns(df: pd.DataFrame) -> dict:
    """
    Best-effort mapping from arbitrary column names to our forensic engine schema.

    Looks for keywords like 'time', 'date', 'amount', 'id', and 'user' and
    returns a mapping for at least:
      - transaction_id (if possible)
      - lead_time (first time/date-like column)
      - payment_time (second time/date-like column)
    """
    columns_lower = {c.lower(): c for c in df.columns}

    # Detect ID-like column (any column containing 'id' or 'trans')
    txn_id_col = None
    for key, orig in columns_lower.items():
        if "id" in key or "trans" in key:
            txn_id_col = orig
            break

    # Detect time/date-like columns (including explicit known names)
    time_cols = [
        orig
        for key, orig in columns_lower.items()
        if ("time" in key) or ("date" in key)
    ]

    mapping: dict[str, str | None] = {
        "transaction_id": txn_id_col,
        "lead_time": None,
        "payment_time": None,
    }

    # Prefer explicit Request_Start / Completion_Timestamp style names when present
    for key, orig in columns_lower.items():
        if "request_start" in key:
            mapping["lead_time"] = orig
        if "completion_timestamp" in key:
            mapping["payment_time"] = orig

    # Fallback to first/second time-like columns if still unset
    if mapping["lead_time"] is None and len(time_cols) >= 1:
        mapping["lead_time"] = time_cols[0]
    if mapping["payment_time"] is None and len(time_cols) >= 2:
        mapping["payment_time"] = time_cols[1]

    return mapping


def _benford_summary(df: pd.DataFrame) -> str:
    """
    Compute a simple Benford-style summary over any amount-like column, if present.
    """
    amount_col = None
    for col in df.columns:
        lower = col.lower()
        if "amount" in lower or "amt" in lower or "value" in lower:
            amount_col = col
            break

    if amount_col is None:
        return "Benford's Law: Skipped (no amount/value column detected)."

    series = pd.to_numeric(df[amount_col], errors="coerce").dropna()
    if series.empty:
        return "Benford's Law: Skipped (amount column contains no numeric values)."

    first_digits = series.astype(str).str.lstrip("0.-").str[0].astype(int)
    counts = first_digits.value_counts(normalize=True).sort_index()

    theoretical = {d: (pd.np.log10(1 + 1 / d)) for d in range(1, 10)}

    lines = [f"Benford's Law on column '{amount_col}':"]
    for d in range(1, 10):
        observed = counts.get(d, 0.0)
        expected = theoretical[d]
        lines.append(
            f"  Digit {d}: observed={observed:.3f}, expected={expected:.3f}, "
            f"delta={observed - expected:+.3f}"
        )

    return "\n".join(lines)


def build_audit_pdf(
    result: dict,
    ai_text: str,
    fraud_df: pd.DataFrame,
    full_df: pd.DataFrame | None = None,
    potential_haircut: int = 0,
) -> bytes:
    """
    Build the official Axiom Forensics Truth OS PDF report.
    """
    pdf = AxiomPDF()
    pdf.set_auto_page_break(auto=True, margin=15)
    pdf.add_page()

    # Title
    pdf.set_font("Helvetica", "B", 16)
    pdf.cell(0, 10, "AXIOM FORENSICS - TRUTH OS REPORT", ln=True, align="C")
    pdf.ln(5)

    # Summary section
    pdf.set_font("Helvetica", "B", 12)
    pdf.cell(0, 8, "Summary", ln=True)
    pdf.set_font("Helvetica", "", 11)
    pdf.multi_cell(
        0,
        6,
        f"Portfolio Integrity Score: {result['integrity_score']:.2f}%\n"
        f"Total Transactions: {result['total_rows']}\n"
        f"Fraud-Flagged Transactions: {result['fraud_rows']}\n",
    )
    pdf.ln(4)

    # Forensic Valuation Summary
    pdf.set_font("Helvetica", "B", 12)
    pdf.cell(0, 8, "Forensic Valuation Summary", ln=True)
    pdf.set_font("Helvetica", "", 11)
    total_deal_value = 300_000_000  # $300M
    pdf.multi_cell(
        0,
        6,
        f"Total Deal Value: ${total_deal_value:,.0f}\n"
        f"Potential Haircut: ${potential_haircut:,.0f}\n"
        f"Haircut Percentage: {(potential_haircut / total_deal_value * 100):.2f}%\n"
        f"Risk Level: {'HIGH' if potential_haircut > 10_000_000 else 'MODERATE' if potential_haircut > 0 else 'LOW'}",
    )
    pdf.ln(4)

    # Mathematical Findings
    pdf.set_font("Helvetica", "B", 12)
    pdf.cell(0, 8, "Mathematical Findings", ln=True)
    pdf.set_font("Helvetica", "", 11)
    benford_text = (
        _benford_summary(full_df) if full_df is not None else "Benford's Law: Dataset not provided."
    )
    pdf.multi_cell(0, 6, benford_text)
    pdf.ln(4)

    # Adversarial AI Report
    pdf.set_font("Helvetica", "B", 12)
    pdf.cell(0, 8, "Adversarial AI Report", ln=True)
    pdf.set_font("Helvetica", "", 11)
    pdf.multi_cell(0, 6, ai_text)
    pdf.ln(4)

    # Evidence section
    pdf.set_font("Helvetica", "B", 12)
    pdf.cell(0, 8, "Evidence: Flagged Transactions", ln=True)
    pdf.set_font("Helvetica", "", 11)

    if fraud_df.empty:
        pdf.multi_cell(0, 6, "No fraud-flagged transactions in this portfolio.")
    else:
        flagged_ids = (
            fraud_df["transaction_id"].astype(int).head(10).tolist()
        )
        ids_str = ", ".join(str(tid) for tid in flagged_ids)
        pdf.multi_cell(
            0,
            6,
            "Top flagged Transaction IDs (max 10): " + ids_str,
        )

    # Export PDF to bytes
    pdf_bytes = pdf.output(dest="S").encode("latin-1")
    return pdf_bytes


def _handle_login():
    """Handle login logic"""
    username = st.session_state.get("username_input", "")
    password = st.session_state.get("password", "")
    if username == DEMO_USERNAME and password == DEMO_PASSWORD:
        st.session_state["password_correct"] = True
        # Do not keep the raw secrets in memory
        st.session_state.pop("password", None)
        st.session_state.pop("username_input", None)
    else:
        st.session_state["password_correct"] = False


def main():
    st.set_page_config(page_title="Axiom Forge Truth OS", layout="centered")

    # Authentication check at the very beginning
    if not st.session_state.get("password_correct", False):
        st.title("Axiom Forge Truth OS")
        st.subheader("Chronos-Audit Portfolio Dashboard")
        
        # Show login form
        st.sidebar.markdown("### Login")
        st.sidebar.text_input(
            "Username",
            key="username_input",
        )
        st.sidebar.text_input(
            "Password",
            type="password",
            key="password",
            on_change=_handle_login
        )

        if st.session_state.get("password_correct") is False and st.session_state.get("password"):
            st.sidebar.error("Access Denied. Incorrect password.")

        st.sidebar.caption(
            "Forgot Password? Please contact the Axiom System Administrator for access credentials."
        )

        st.write(
            "Status: üîí Locked. Enter the access password in the sidebar to unlock "
            "the Axiom Forensics dashboard."
        )
        
        # Stop execution if not authenticated
        st.stop()

    # Dashboard logic only runs if authenticated
    st.title("Axiom Forge Truth OS")
    st.subheader("Chronos-Audit Portfolio Dashboard")

    st.write(
        "Run a latency-based integrity sweep across the current transaction portfolio "
        "to surface zero-latency fraud paradoxes."
    )

    uploaded_file = st.sidebar.file_uploader(
        "Optional: Upload CSV for Audit", type=["csv"]
    )

    # üõ†Ô∏è Forensic Toolbox Section
    if uploaded_file is not None:
        st.markdown("---")
        st.subheader("üõ†Ô∏è Forensic Metadata & Integrity")
        
        # Calculate file hashes
        file_content = uploaded_file.getvalue()
        hashes = calculate_file_hashes(file_content)
        
        # Display file integrity information
        col1, col2 = st.columns(2)
        with col1:
            st.write("**File Integrity Hashes**")
            st.code(f"MD5: {hashes['md5']}")
            st.code(f"SHA-256: {hashes['sha256']}")
        
        # Extract metadata based on file type
        file_type = get_file_type(file_content, uploaded_file.name)
        
        if file_type == 'pdf':
            with col2:
                st.write("**PDF Metadata**")
                pdf_metadata = extract_pdf_metadata(file_content)
                if 'error' in pdf_metadata:
                    st.error(pdf_metadata['error'])
                else:
                    metadata_text = ""
                    if pdf_metadata['author']:
                        metadata_text += f"**Author:** {pdf_metadata['author']}\n"
                    if pdf_metadata['creator']:
                        metadata_text += f"**Creator:** {pdf_metadata['creator']}\n"
                    if pdf_metadata['creation_date']:
                        metadata_text += f"**Creation Date:** {pdf_metadata['creation_date']}\n"
                    if pdf_metadata['title']:
                        metadata_text += f"**Title:** {pdf_metadata['title']}\n"
                    if pdf_metadata['page_count']:
                        metadata_text += f"**Page Count:** {pdf_metadata['page_count']}\n"
                    
                    if metadata_text:
                        st.markdown(metadata_text)
                    else:
                        st.info("No metadata found in this PDF.")
        
        elif file_type == 'image':
            with col2:
                st.write("**Image Metadata (EXIF)**")
                image_metadata = extract_image_metadata(file_content)
                if 'error' in image_metadata:
                    st.error(image_metadata['error'])
                else:
                    metadata_text = ""
                    if image_metadata['format']:
                        metadata_text += f"**Format:** {image_metadata['format']}\n"
                    if image_metadata['size']:
                        metadata_text += f"**Size:** {image_metadata['size'][0]}x{image_metadata['size'][1]}\n"
                    if image_metadata['mode']:
                        metadata_text += f"**Color Mode:** {image_metadata['mode']}\n"
                    
                    if metadata_text:
                        st.markdown(metadata_text)
                    
                    # Show some key EXIF data
                    exif_data = image_metadata.get('exif_data', {})
                    if exif_data:
                        st.write("**Key EXIF Data:**")
                        key_exif = ['DateTime', 'Make', 'Model', 'Software', 'GPSInfo']
                        exif_text = ""
                        for key in key_exif:
                            if key in exif_data:
                                exif_text += f"**{key}:** {exif_data[key]}\n"
                        
                        if exif_text:
                            st.markdown(exif_text)
                        else:
                            st.info("No key EXIF data found.")
                    else:
                        st.info("No EXIF data found in this image.")
        
        else:
            with col2:
                st.info(f"File type '{file_type}' not supported for metadata extraction. Hashes calculated for integrity verification.")

    # Sandbox Tools: in-app demo data generators
    st.sidebar.markdown("### Sandbox Tools")

    if st.sidebar.button("Generate Clean Data"):
        rows = []
        for i in range(1, 1001):
            lead_time = float(i * 100)
            gap = random.uniform(2.0, 10.0)
            payment_time = lead_time + gap
            rows.append(
                {
                    "transaction_id": i,
                    "lead_time": lead_time,
                    "payment_time": payment_time,
                }
            )
        st.session_state["sandbox_df"] = pd.DataFrame(rows)
        st.session_state["sandbox_label"] = "Sandbox: Clean Data"
        st.session_state["sandbox_mode"] = "clean"

    if st.sidebar.button("Generate Attack Data"):
        rows = []
        for i in range(1, 1001):
            lead_time = float(i * 100)
            # 40% of rows with 0.1s latency paradox
            if random.random() < 0.4:
                gap = 0.1
            else:
                gap = random.uniform(2.0, 10.0)
            payment_time = lead_time + gap
            rows.append(
                {
                    "transaction_id": i,
                    "lead_time": lead_time,
                    "payment_time": payment_time,
                }
            )
        st.session_state["sandbox_df"] = pd.DataFrame(rows)
        st.session_state["sandbox_label"] = "Sandbox: Attack Data (Latency Paradox)"
        st.session_state["sandbox_mode"] = "attack"

    if st.button("Run Portfolio Audit"):
        # Decide data source: sandbox data, uploaded file, or demo dataset
        sandbox_df = st.session_state.get("sandbox_df")
        if sandbox_df is not None:
            df = sandbox_df.copy()
            data_label = st.session_state.get("sandbox_label", "Sandbox data")
        elif uploaded_file is not None:
            try:
                df = pd.read_csv(uploaded_file)
            except Exception:
                st.error(
                    "Unable to read the uploaded file as CSV. "
                    "Please upload a valid comma-separated values (.csv) file."
                )
                return

            if df.empty:
                st.error(
                    "The uploaded CSV contains no rows. "
                    "Please provide a file with at least one transaction row."
                )
                return

            data_label = f"Uploaded file: {uploaded_file.name}"
        else:
            df = pd.read_csv("data/transactions.csv")
            data_label = "Demo dataset: data/transactions.csv"

        # Infer forensic column mapping (Smart Mapper)
        auto_mapping = infer_forensic_columns(df)

        # Column Mapping UI in sidebar (with Smart Mapper + manual override)
        st.sidebar.markdown("### Smart Mapper: Column Mapping")
        st.sidebar.caption(
            "Review or override how Axiom maps your columns to the forensic engine."
        )
        all_cols = list(df.columns)
        options = ["(not selected)"] + all_cols

        def _default_index(target: str | None) -> int:
            if target and target in all_cols:
                return options.index(target)
            return 0

        sel_txn = st.sidebar.selectbox(
            "Transaction ID column",
            options=options,
            index=_default_index(auto_mapping.get("transaction_id")),
        )
        sel_lead = st.sidebar.selectbox(
            "Lead time column",
            options=options,
            index=_default_index(auto_mapping.get("lead_time")),
        )
        sel_payment = st.sidebar.selectbox(
            "Payment time column",
            options=options,
            index=_default_index(auto_mapping.get("payment_time")),
        )

        # Normalize "(not selected)" to None
        sel_txn = None if sel_txn == "(not selected)" else sel_txn
        sel_lead = None if sel_lead == "(not selected)" else sel_lead
        sel_payment = None if sel_payment == "(not selected)" else sel_payment

        # Final mapping after user overrides
        mapping = {
            "transaction_id": sel_txn,
            "lead_time": sel_lead,
            "payment_time": sel_payment,
        }

        # Show explicit mapping text
        st.sidebar.write(
            f'Mapping "transaction_id" to your column "{mapping["transaction_id"]}"'
            if mapping["transaction_id"]
            else 'Mapping "transaction_id" is not set.'
        )
        st.sidebar.write(
            f'Mapping "lead_time" to your column "{mapping["lead_time"]}"'
            if mapping["lead_time"]
            else 'Mapping "lead_time" is not set.'
        )
        st.sidebar.write(
            f'Mapping "payment_time" to your column "{mapping["payment_time"]}"'
            if mapping["payment_time"]
            else 'Mapping "payment_time" is not set.'
        )

        # Validate required mappings
        if mapping["lead_time"] is None or mapping["payment_time"] is None:
            st.warning(
                "Please select both a lead time and a payment time column in the "
                "Column Mapping section to run the audit."
            )
            return
        if mapping["transaction_id"] is None:
            st.warning(
                "Please select a transaction ID column in the Column Mapping section "
                "to run the audit."
            )
            return

        # Safety check: ensure selected time columns are numeric/datetime-like
        lead_series = df[mapping["lead_time"]]
        payment_series = df[mapping["payment_time"]]

        if not (
            pd.api.types.is_numeric_dtype(lead_series)
            or pd.api.types.is_datetime64_any_dtype(lead_series)
        ) or not (
            pd.api.types.is_numeric_dtype(payment_series)
            or pd.api.types.is_datetime64_any_dtype(payment_series)
        ):
            st.error(
                "Invalid Data Type: Please ensure your selected time columns contain numbers."
            )
            return

        # Normalize to engine schema
        normalized_df = df.copy()
        if mapping["transaction_id"] != "transaction_id":
            normalized_df = normalized_df.rename(
                columns={mapping["transaction_id"]: "transaction_id"}
            )
        if mapping["lead_time"] != "lead_time":
            normalized_df = normalized_df.rename(
                columns={mapping["lead_time"]: "lead_time"}
            )
        if mapping["payment_time"] != "payment_time":
            normalized_df = normalized_df.rename(
                columns={mapping["payment_time"]: "payment_time"}
            )

        with st.spinner("Running Chronos-Audit over selected dataset..."):
            # Progress bar for large datasets
            total_rows = len(normalized_df)
            progress = st.progress(0)

            result = run_portfolio_audit(df=normalized_df)

            # Re-run per-transaction checks to build an evidence locker
            fraud_flags = []
            remediation_tips = []

            for idx, (_, row) in enumerate(normalized_df.iterrows(), start=1):
                res = check_timestamps(
                    lead_created_at=row["lead_time"],
                    payment_processed_at=row["payment_time"],
                )
                fraud_flags.append(res["verdict"].startswith("FRAUD ALERT"))
                remediation_tips.append(res["remediation_tip"])

                if total_rows:
                    progress.progress(min(idx / total_rows, 1.0))

            normalized_df["is_fraud"] = fraud_flags
            normalized_df["remediation_tip"] = remediation_tips
            fraud_df = normalized_df[normalized_df["is_fraud"]].copy()

        st.success("Portfolio audit completed successfully.")

        # Hero Section: Forensic Integrity Gauge + Risk Verdict
        hero_col_gauge, hero_col_text = st.columns([2, 1])
        score = result["integrity_score"]

        # Determine color bands and risk verdict
        if score >= 90:
            risk_label = "Low Risk"
            risk_color = "green"
        elif score >= 70:
            risk_label = "Caution"
            risk_color = "gold"
        else:
            risk_label = "High Fraud Probability"
            risk_color = "red"

        gauge_fig = go.Figure(
            go.Indicator(
                mode="gauge+number",
                value=score,
                title={"text": "Forensic Integrity Score"},
                gauge={
                    "axis": {"range": [0, 100]},
                    "bar": {"color": risk_color},
                    "steps": [
                        {"range": [0, 70], "color": "#ffcccc"},
                        {"range": [70, 90], "color": "#fff4c2"},
                        {"range": [90, 100], "color": "#d4f4dd"},
                    ],
                },
            )
        )

        with hero_col_gauge:
            st.plotly_chart(gauge_fig, use_container_width=True)

        with hero_col_text:
            st.markdown("### Risk Verdict")
            st.markdown(f"**{risk_label}**")
            st.markdown(
                f"Current portfolio integrity is **{score:.2f}%**, "
                "based on Chronos-Audit latency analysis."
            )

        st.markdown("---")
        st.subheader("Data Preview")
        st.caption(data_label)
        st.dataframe(df.head(5), use_container_width=True)

        st.markdown("---")
        st.subheader("Portfolio Integrity Score")
        st.metric(
            label="Integrity Score",
            value=f"{result['integrity_score']:.2f}%",
            delta=f"-{result['fraud_rows']} fraud-flagged"
            if result["fraud_rows"] > 0
            else None,
        )

        st.write(f"**Total Transactions Audited:** {result['total_rows']}")
        st.write(f"**Fraud-Flagged Transactions:** {result['fraud_rows']}")

        if result["warning"]:
            st.error(
                "Solution Architect Warning\n\n"
                f"{result['warning']}",
                icon="‚ö†Ô∏è",
            )

        with st.spinner("Consulting Axiom AI Brain..."):
            ai_text = get_ai_insights(
                result=result,
                fraud_df=fraud_df,
                dataset_label=data_label,
                row_count=len(normalized_df),
            )

        st.info(ai_text)

        st.markdown("---")
        st.subheader("Fraud Evidence Locker")

        if fraud_df.empty:
            st.info("No fraud-flagged transactions detected in the current portfolio.")
        else:
            # Highlight lead_time and payment_time to show the 0.1s gap clearly
            display_cols = ["transaction_id", "lead_time", "payment_time"]
            styled = fraud_df[display_cols].style.set_properties(
                subset=["lead_time", "payment_time"],
                **{"background-color": "#ffe6e6", "font-weight": "bold"},
            )
            st.dataframe(styled, use_container_width=True)

            # Per-transaction expanders with remediation tips
            for _, row in fraud_df.iterrows():
                header = f"Transaction {int(row['transaction_id'])} ‚Äì Detailed View"
                with st.expander(header):
                    st.write(f"**Lead Time:** {row['lead_time']}")
                    st.write(f"**Payment Time:** {row['payment_time']}")
                    st.write(
                        f"**Latency:** {row['payment_time'] - row['lead_time']:.3f} seconds"
                    )
                    st.write(f"**Remediation Tip:** {row['remediation_tip']}")

        st.markdown("---")

        # üí∞ Forensic Valuation Impact Section
        st.subheader("üí∞ Forensic Valuation Impact")

        # Calculate metrics
        total_deal_value = 300_000_000  # $300M
        high_risk_count = len(fraud_df[fraud_df['risk_score'] > 80]) if 'risk_score' in fraud_df.columns else 0
        potential_haircut = high_risk_count * 300_000  # $300k per flagged item

        # Display metrics in two columns
        col1, col2 = st.columns(2)
        with col1:
            st.metric(
                label="High-Risk Items",
                value=f"{high_risk_count}",
                delta=f"{(high_risk_count / len(normalized_df) * 100):.1f}%" if len(normalized_df) > 0 else "0%"
            )
        with col2:
            st.metric(
                label="Potential Haircut",
                value=f"${potential_haircut:,.0f}",
                delta=f"{(potential_haircut / total_deal_value * 100):.1f}% of deal"
            )

        # Warning if haircut exceeds $10M
        if potential_haircut > 10_000_000:
            st.warning(
                f"‚ö†Ô∏è **High Risk Alert**: Potential haircut of ${potential_haircut:,.0f} "
                f"exceeds the $10M threshold. Consider deeper due diligence."
            )

        st.markdown("---")

        pdf_bytes = build_audit_pdf(result, ai_text, fraud_df, full_df=normalized_df, potential_haircut=potential_haircut)
        timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        filename = f"Axiom_Audit_{timestamp}.pdf"
        st.download_button(
            label="Download Official Audit PDF",
            data=pdf_bytes,
            file_name=filename,
            mime="application/pdf",
            icon="üìÑ",
        )

    # Global reset and logout controls at bottom of sidebar
    if st.sidebar.button("Clear All Data & Reset"):
        st.session_state.clear()
        st.rerun()

    if st.sidebar.button("Logout"):
        st.session_state.clear()
        st.rerun()


if __name__ == "__main__":
    main()

